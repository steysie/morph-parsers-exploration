{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "import junky\n",
    "from corpuscula.corpus_utils import syntagrus\n",
    "\n",
    "from collections import OrderedDict, Counter\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=================================================] 48814           \n",
      "Corpus has been loaded: 48814 sentences, 871526 tokens\n",
      "Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n",
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    }
   ],
   "source": [
    "junky.clear_tqdm()\n",
    "train, train_lemmas = junky.get_conllu_fields(syntagrus.train, fields=['LEMMA'])\n",
    "dev, dev_lemmas = junky.get_conllu_fields(syntagrus.dev, fields=['LEMMA'])\n",
    "test, test_lemmas = junky.get_conllu_fields(syntagrus.test, fields=['LEMMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_corpus = train + train_lemmas + dev + dev_lemmas + test + test_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2freq = dict(Counter(list(chain.from_iterable(full_corpus))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2freq = OrderedDict(sorted(word2freq.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {k: v for k, v in points.items() if v > 888}\n",
    "word2freq = OrderedDict(sorted({k: v for k, v in word2freq.items() if v>888}.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('государство', 904),\n",
       " ('работать', 905),\n",
       " ('случай', 906),\n",
       " ('всегда', 908),\n",
       " ('да', 909),\n",
       " ('конечно', 912),\n",
       " ('здесь', 932),\n",
       " ('потом', 936),\n",
       " ('тем', 939),\n",
       " ('решение', 940)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2freq.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2freq = OrderedDict(\n",
    "    sorted(\n",
    "        Counter(\n",
    "            chain.from_iterable(full_corpus)).items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 284 ms, sys: 0 ns, total: 284 ms\n",
      "Wall time: 280 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2freq = OrderedDict(\n",
    "    sorted(\n",
    "        {k: v for k, v in Counter(\n",
    "            chain.from_iterable(full_corpus)).items() if v>888}.items(), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 404 ms, sys: 0 ns, total: 404 ms\n",
      "Wall time: 401 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "word2freq = OrderedDict(\n",
    "    sorted(\n",
    "        filter(lambda x: x[1] > 0.0, \n",
    "                    Counter(chain.from_iterable(full_corpus)).items()), key=lambda t: t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('разгорелось', 1),\n",
       " ('подтекста', 1),\n",
       " ('раллиста', 1),\n",
       " ('Тристрама', 1),\n",
       " ('ледостойкой', 1),\n",
       " ('Цимлянское', 1),\n",
       " ('верующего', 1),\n",
       " ('сочинена', 1),\n",
       " ('повеселели', 1),\n",
       " ('обошлась', 1)]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2freq.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'разгорелось'"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2freq.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distilling vector vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_embeddings(pretrained_embs, corpus, min_abs_freq=1, save_name=None,\n",
    "                   include_emb_info=False, pad_token=None, unk_token=None,\n",
    "                   extra_tokens=None):\n",
    "    \n",
    "    \"\"\"Filters pretrained word embeddings' vocabulary, leaving only tokens \n",
    "    that are present in the specified `corpus` which are more frequent than\n",
    "    minimum absolute frequency `min_abs_freq`. This method allows to\n",
    "    significantly reduce memory usage and speed up word embedding process.\n",
    "    The drawbacks include lower performance on unseen data.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "    **vectors**: file with pretrained word vectors in text format (not\n",
    "    binary), where the first line is\n",
    "    `<vocab_size> <embedding_dimensionality>`.\n",
    "    \n",
    "    **corpus**: a list of lists or tuples with already tokenized sentences.\n",
    "    Filtered result will not contain any tokens outside of this corpus.\n",
    "    \n",
    "    **min_abs_freq** (`int`): minimum absolute frequency; only tokens the\n",
    "    frequency of which is equal or greater than this specified value will be\n",
    "    included in the filtered word embeddings. Default `min_abs_freq=1`,\n",
    "    meaning all words from the corpus that have corresponding word vectors in\n",
    "    `pretrained_embs` are preserved. \n",
    "    \n",
    "    **save_name**(`str`): if specified, filtered word embeddings are saved in\n",
    "    a file with the specified name.\n",
    "    \n",
    "    **include_emb_info**(`bool`): whether to include `<vocab_size> <emb_dim>`\n",
    "    as the first line to the filtered embeddings file. Default is `False`,\n",
    "    embedding info line is skipped. Relevant only if `save_name` is not None.\n",
    "    \n",
    "    For the arguments below note, that typically pretrained embeddings already\n",
    "    include PAD or UNK tokens. But these argumets are helpful if you need to\n",
    "    specify your custom pad/unk/extra tokens or make sure they are at the top\n",
    "    of the vocab (thus, pad_token will have index=0 for convenience).\n",
    "    \n",
    "    **pad_token** (`str`): custom padding token, which is initialized with\n",
    "    zeros and included at the top of the vocabulary. \n",
    "    \n",
    "    **unk_token** (`str`): custom token for unknown words, which is\n",
    "    initialized with small random numbers and included at the top of the\n",
    "    vocabulary.\n",
    "    \n",
    "    **extra_tokens** (`list`): list of any extra custom tokens. For now, they\n",
    "    are initialized with small random numbers and included at the top of the\n",
    "    vocabulary. Typically, used for special tokens, e.g. start/end tokens etc.\n",
    "    \n",
    "    If `save_name` is specified, saves the filtered vocabulary. Otherwise,\n",
    "    returns word2index OrderedDict and a numpy array of corresponding word\n",
    "    vectors.\n",
    "    \"\"\"\n",
    "    \n",
    "    filter_vocab = OrderedDict(\n",
    "        sorted(\n",
    "            {k: v \n",
    "             for k, v in Counter(chain.from_iterable(corpus)).items()\n",
    "             if v>=min_abs_freq}.items(), \n",
    "            key=lambda t: t[1]))\n",
    "    \n",
    "    word2index = OrderedDict()\n",
    "    vectors = []\n",
    "\n",
    "    # model in vec or txt format\n",
    "    # (not binary, first line is <vocab_size> <emb_dim>)\n",
    "    word2vec_file = open(pretrained_embs)\n",
    "    \n",
    "    n_words, embedding_dim = word2vec_file.readline().split()\n",
    "    n_words, embedding_dim = int(n_words), int(embedding_dim)\n",
    "    \n",
    "    if pad_token:\n",
    "        # Zero vector for PAD\n",
    "        vectors.append(np.zeros((1, embedding_dim)))\n",
    "        word2index[pad_token] = len(word2index)\n",
    "        \n",
    "    if unk_token:\n",
    "        # Initializing UNK vector with small random numbers \n",
    "        vectors.append(\n",
    "            np.random.rand(1, embedding_dim) / math.sqrt(embedding_dim))\n",
    "        word2index[unk_token] = len(word2index)\n",
    "    \n",
    "    if extra_tokens:\n",
    "        # random-small-number vectors for extra_tokens\n",
    "        for x_t in extra_tokens:\n",
    "            vectors.append(\n",
    "                np.random.rand(1, embedding_dim) / math.sqrt(embedding_dim))\n",
    "            word2index[x_t] = len(word2index)\n",
    "\n",
    "    progress_bar = tqdm(desc='Filtering vectors', total=n_words)\n",
    "\n",
    "    while True:\n",
    "        line = word2vec_file.readline().strip()\n",
    "\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        current_parts = line.split()\n",
    "        current_word = ' '.join(current_parts[:-embedding_dim])\n",
    "\n",
    "        if current_word in filter_vocab:\n",
    "\n",
    "            word2index[current_word] = len(word2index)\n",
    "\n",
    "            current_vectors = current_parts[-embedding_dim:]\n",
    "            current_vectors = np.array(list(map(float, current_vectors)))\n",
    "            current_vectors = np.expand_dims(current_vectors, 0)\n",
    "\n",
    "            vectors.append(current_vectors)\n",
    "            \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    progress_bar.close()\n",
    "    word2vec_file.close()\n",
    "\n",
    "    vectors = list(np.concatenate(vectors))\n",
    "    \n",
    "    if save_name:\n",
    "        with open(save_name, 'w') as f:\n",
    "            if include_emb_info:\n",
    "                print(len(word2index), embedding_dim, file=f)\n",
    "        \n",
    "            for word, vector in tqdm(zip(word2index.keys(), vectors), \n",
    "                         desc='Saving filtered vectors', total=len(vectors)):\n",
    "                print(word, ' '.join(str(v) for v in vector),\n",
    "                      end=' \\n', file=f)\n",
    "    else:\n",
    "        return word2index, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering vectors: 100%|██████████| 1560131/1560131 [00:34<00:00, 45728.89it/s]\n"
     ]
    }
   ],
   "source": [
    "FT_VECTORS_PATH = 'ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec'\n",
    "\n",
    "w2x, vects = filter_embeddings(pretrained_embs=FT_VECTORS_PATH, corpus=full_corpus, min_abs_freq=7000,\n",
    "                         # save_name='filtered_vectors_freq1.vec', \n",
    "                               include_emb_info=True,\n",
    "                         pad_token='[PAD]', unk_token='[UNK]', extra_tokens=['[START]', '[END]']\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array([0., 0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array2string(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0 0.0 0.0 0.0 0.0'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(str(i) for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128692"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename = 'filtered_vectors.vec'\n",
    "filename = 'filtered_vectors_freq1.vec'\n",
    "# filename = 'ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec'\n",
    "\n",
    "len(open(filename).readlines())\n",
    "# close(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 \\n'"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[PAD] 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\\n'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.readline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
